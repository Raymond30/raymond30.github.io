# jemdoc: menu{MENU}{index.html}

= Ruichen Jiang

~~~
{}{img_left}{photos/headshot.jpg}{A photo of myself}{218}{221}{https://raymond30.github.io/}
Ph.D. student, \n
[https://www.ece.utexas.edu/ Department of Electrical and Computer Engineering], \n
[https://www.utexas.edu/ The University of Texas at Austin] \n
# Phone: +86 13683258743 \n
E-mail: rjiang \[at\] utexas \[dot\] edu \n
[https://scholar.google.com/citations?hl=en&user=BGFt1UMAAAAJ&view_op=list_works&sortby=pubdate Google Scholar]
# [Ruichen_Jiang_CV_2019.pdf A brief CV is here].
~~~

== About me
I am a 5th-year Ph.D. student in the Department of ECE at UT Austin, advised by [https://sites.utexas.edu/mokhtari/ Prof. Aryan Mokhtari]. 
Before UT, I received a B.E. degree in Electronic Engineering and a B.S. degree in Mathematics both from [https://www.tsinghua.edu.cn/en/ Tsinghua University] in 2020.  

My current research interests focus on *convex* and *non-convex optimization*, particularly in using *online learning* techniques to design optimization methods. Recently, I have been working on min-max optimization, second-order and higher-order methods, and bilevel optimization. 

== Selected Works

- [https://arxiv.org/abs/2306.02212 Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization] \n
*Ruichen Jiang* and Aryan Mokhtari \n
NeurIPS 2023 (*Spotlight*)

- [https://arxiv.org/abs/2302.08580 Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence] \n
*Ruichen Jiang*, Qiujiang Jin, and Aryan Mokhtari \n
COLT 2023

- [https://arxiv.org/abs/2206.08868 A Conditional Gradient-based Method for Simple Bilevel Optimization with Convex Lower-level Problem] \n
    *Ruichen Jiang*, Nazanin Abolfazli, Aryan Mokhtari, and Erfan Yazdandoost Hamedani \n
    AISTATS 2023

- [https://arxiv.org/abs/2202.09674 Generalized Optimistic Methods for Convex-Concave Saddle Point Problems] \n
    *Ruichen Jiang* and Aryan Mokhtari \n
    ArXiv preprint, 2022

== All Conference Papers

- [https://arxiv.org/abs/2401.03058 Krylov Cubic Regularized Newton: A Subspace Second-Order Method with Dimension-Free Convergence Rate] \n
*Ruichen Jiang*, Parameswaran Raman, Shoham Sabach, Aryan Mokhtari, Mingyi Hong, and Volkan Cevher \n
AISTATS 2024

- [https://arxiv.org/abs/2306.02212 Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization] \n
*Ruichen Jiang* and Aryan Mokhtari \n
NeurIPS 2023 (*Spotlight*)

- [https://arxiv.org/abs/2308.07536 Projection-Free Methods for Stochastic Simple Bilevel Optimization with Convex Lower-level Problem] \n
Jincheng Cao, *Ruichen Jiang*, Nazanin Abolfazli, Erfan Yazdandoost Hamedani, and Aryan Mokhtari \n
NeurIPS 2023

- [https://arxiv.org/abs/2302.08580 Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence] \n
 *Ruichen Jiang*, Qiujiang Jin, and Aryan Mokhtari \n
  COLT 2023

- [https://arxiv.org/abs/2206.08868 A Conditional Gradient-based Method for Simple Bilevel Optimization with Convex Lower-level Problem] \n
    *Ruichen Jiang*, Nazanin Abolfazli, Aryan Mokhtari, and Erfan Yazdandoost Hamedani \n
    AISTATS 2023

- [https://openreview.net/forum?id=H34Ah8Loqgq Future Gradient Descent for Adapting the Temporal Shifting Data Distribution in Online Recommendation System] \n
    Mao Ye, *Ruichen Jiang*, Haoxiang Wang, Dhruv Choudhary, Xiaocong Du, Bhargav Bhushanam, Aryan Mokhtari, Arun Kejariwal, and Qiang Liu \n
    UAI 2022

- [https://arxiv.org/abs/2104.11549 Antenna Efficiency in Massive MIMO Detection] \n
    *Ruichen Jiang* and Ya-Feng Liu \n
    IEEE SPAWC 2021
    #, Sept. 2021

- [https://ieeexplore.ieee.org/document/9348844 Achieving Cooperative Diversity in Over-the-Air Computation via Relay Selection] \n
    *Ruichen Jiang*, Sheng Zhou, and Kaibin Huang \n
    IEEE VTC2020-Fall 2020
    #, Nov. 2020

- [https://arxiv.org/abs/2008.00994 Cluster-Based Cooperative Digital Over-the-Air Aggregation for Wireless Federated Edge Learning] \n
    *Ruichen Jiang* and Sheng Zhou \n
    IEEE/CIC ICCC 2020
    #, Aug. 2020

== All Preprints

- [https://arxiv.org/abs/2406.04592 Convergence Analysis of Adaptive Gradient Methods under Refined Smoothness and Noise Assumptions] \n
Devyani Maladkar, *Ruichen Jiang*, Aryan Mokhtari \n
ArXiv preprint, 2024

- [https://arxiv.org/abs/2406.02016 Adaptive and Optimal Second-order Optimistic Methods for Minimax Optimization] \n
*Ruichen Jiang*, Ali Kavis, Qiujiang Jin, Sujay Sanghavi, Aryan Mokhtari \n
ArXiv preprint, 2024

- [https://arxiv.org/abs/2406.01478 Stochastic Newton Proximal Extragradient Method] \n
*Ruichen Jiang*, Michał Dereziński, Aryan Mokhtari \n
ArXiv preprint, 2024

- [https://arxiv.org/abs/2404.16731 Non-asymptotic Global Convergence Analysis of BFGS with the Armijo-Wolfe Line Search] \n
Qiujiang Jin, *Ruichen Jiang*, Aryan Mokhtari \n
ArXiv preprint, 2024

- [https://arxiv.org/abs/2404.01267 Non-asymptotic Global Convergence Rates of BFGS with Exact Line Search] \n
Qiujiang Jin, *Ruichen Jiang*, Aryan Mokhtari \n
ArXiv preprint, 2024

- [https://arxiv.org/abs/2402.08097 An Accelerated Gradient Method for Simple Bilevel Optimization with Convex Lower-level Problem] \n
Jincheng Cao, *Ruichen Jiang*, Erfan Yazdandoost Hamedani, Aryan Mokhtari \n
ArXiv preprint, 2024

- [https://arxiv.org/abs/2306.02429 An Inexact Conditional Gradient Method for Constrained Bilevel Optimization] \n
Nazanin Abolfazli, *Ruichen Jiang*, Aryan Mokhtari, and Erfan Yazdandoost Hamedani  \n
ArXiv preprint, 2023

- [https://arxiv.org/abs/2202.09674 Generalized Optimistic Methods for Convex-Concave Saddle Point Problems] \n
    *Ruichen Jiang* and Aryan Mokhtari \n
    ArXiv preprint, 2022

- [https://arxiv.org/abs/2102.04586 Tightness and Equivalence of Semidefinite Relaxations for MIMO Detection] \[[http://lsec.cc.ac.cn/~yafliu/Technical_Report_MIMO.pdf Companion Report]\] \n
    *Ruichen Jiang*, Ya-Feng Liu, Chenglong Bao, and Bo Jiang \n
    ArXiv preprint, 2021
